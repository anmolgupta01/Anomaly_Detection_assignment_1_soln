{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0caaff15",
   "metadata": {},
   "source": [
    "# Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6882a",
   "metadata": {},
   "source": [
    "Anomaly detection is a technique in data science that focuses on identifying patterns, instances, or observations that deviate significantly from the expected behavior within a dataset. The purpose of anomaly detection is to pinpoint unusual or rare events, patterns, or outliers that may indicate potential issues, errors, fraud, or interesting insights in the data.\n",
    "\n",
    "In various fields such as finance, cybersecurity, manufacturing, healthcare, and more, anomaly detection plays a crucial role in improving the quality of decision-making processes. Here are some key aspects of anomaly detection:\n",
    "\n",
    "1. **Unsupervised Learning:** Anomaly detection often involves unsupervised learning techniques, as anomalies are typically not explicitly labeled in the training data. The algorithm learns the normal behavior from the majority of the data and then identifies instances that deviate significantly.\n",
    "\n",
    "2. **Types of Anomalies:** Anomalies can take different forms, including point anomalies (individual data points that are unusual), contextual anomalies (instances that are anomalous in a specific context), and collective anomalies (groups of data points exhibiting unusual behavior when considered together).\n",
    "\n",
    "3. **Applications:**\n",
    "   - **Fraud Detection:** Identifying fraudulent activities in financial transactions.\n",
    "   - **Network Security:** Detecting unusual patterns in network traffic that may indicate a cyber attack.\n",
    "   - **Manufacturing Quality Control:** Identifying defective products on a production line.\n",
    "   - **Health Monitoring:** Detecting abnormal physiological readings in healthcare data.\n",
    "   - **Predictive Maintenance:** Identifying equipment failures or malfunctions in advance.\n",
    "\n",
    "4. **Methods:**\n",
    "   - **Statistical Methods:** Using statistical techniques such as mean, median, standard deviation, or z-scores to identify deviations from the norm.\n",
    "   - **Machine Learning Algorithms:** Employing machine learning models like isolation forests, one-class SVM, or autoencoders to learn normal patterns and identify anomalies.\n",
    "\n",
    "5. **Challenges:** Anomaly detection faces challenges such as imbalanced datasets, evolving patterns of normal behavior, and the definition of what constitutes an anomaly can vary depending on the context.\n",
    "\n",
    "By highlighting unusual patterns or events, anomaly detection helps organizations make informed decisions, improve system reliability, enhance security, and ultimately save resources by addressing issues before they escalate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e824d",
   "metadata": {},
   "source": [
    "# Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d2fce",
   "metadata": {},
   "source": [
    "Anomaly detection is a valuable tool in various domains, but it comes with several challenges that researchers and practitioners need to address for effective implementation. Some key challenges in anomaly detection include:\n",
    "\n",
    "1. **Imbalanced Datasets:**\n",
    "   - In many real-world scenarios, anomalies are rare compared to normal instances. This class imbalance can lead to models biased towards the majority class, making it challenging to detect the minority class anomalies effectively.\n",
    "\n",
    "2. **Dynamic and Evolving Patterns:**\n",
    "   - Normal behavior in a system can change over time, and anomalies may evolve as well. Static models may struggle to adapt to these dynamic patterns, requiring continuous monitoring and updating of the anomaly detection system.\n",
    "\n",
    "3. **Noisy Data:**\n",
    "   - Datasets may contain noise or outliers that are not true anomalies but can influence the performance of anomaly detection algorithms. Preprocessing techniques are often required to handle noisy data effectively.\n",
    "\n",
    "4. **Scalability:**\n",
    "   - Anomaly detection algorithms need to be scalable to handle large datasets and real-time streaming data. Processing high volumes of data efficiently can be a computational challenge.\n",
    "\n",
    "5. **Contextual Anomalies:**\n",
    "   - Identifying anomalies based on context is crucial, as what may be considered anomalous in one context might be normal in another. Incorporating contextual information into the anomaly detection process is a complex task.\n",
    "\n",
    "6. **Adversarial Attacks:**\n",
    "   - In applications like cybersecurity or fraud detection, malicious actors may deliberately try to manipulate data to evade detection. Anomaly detection systems need to be robust against adversarial attacks.\n",
    "\n",
    "7. **Interpretability:**\n",
    "   - Understanding and interpreting the results of anomaly detection algorithms can be challenging, especially with complex machine learning models. Interpretable models are important, especially in critical applications where decisions impact human safety or financial transactions.\n",
    "\n",
    "8. **Data Quality and Missing Values:**\n",
    "   - Anomaly detection models are sensitive to data quality. Missing values or inaccuracies in the data can affect the performance of the algorithm. Preprocessing and data cleaning are essential steps in addressing this challenge.\n",
    "\n",
    "9. **Temporal Dependencies:**\n",
    "   - Anomalies in time-series data may exhibit temporal dependencies, where the occurrence of an anomaly at one time point influences the likelihood of anomalies at subsequent points. Capturing such dependencies requires specialized modeling techniques.\n",
    "\n",
    "10. **Human-in-the-Loop Challenges:**\n",
    "    - In many applications, human expertise is crucial for validating anomalies. Balancing automated detection with human-in-the-loop validation can be challenging, particularly when dealing with a high volume of alerts.\n",
    "\n",
    "Addressing these challenges often involves a combination of domain knowledge, careful feature engineering, choosing appropriate algorithms, and continuous monitoring and adaptation of the anomaly detection system. Researchers and practitioners continue to explore innovative solutions to improve the robustness and effectiveness of anomaly detection techniques in diverse applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed37df8",
   "metadata": {},
   "source": [
    "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73ba67",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches in the field of anomaly detection, and they differ primarily in the way they leverage labeled data during the training phase.\n",
    "\n",
    "1. **Unsupervised Anomaly Detection:**\n",
    "   - **Training Data:** Unsupervised anomaly detection algorithms work with unlabeled data, meaning the training dataset contains instances without explicit annotations indicating whether they are normal or anomalous.\n",
    "   - **Learning Normal Behavior:** The algorithm learns the patterns of normal behavior from the majority of the data without being provided with specific instances of anomalies.\n",
    "   - **Detecting Anomalies:** During the testing or deployment phase, the algorithm identifies instances that deviate significantly from the learned normal patterns as potential anomalies.\n",
    "\n",
    "   **Examples of Unsupervised Methods:**\n",
    "   - Clustering-based methods (e.g., k-means clustering).\n",
    "   - Density-based methods (e.g., DBSCAN - Density-Based Spatial Clustering of Applications with Noise).\n",
    "   - Statistical methods (e.g., using mean, median, standard deviation).\n",
    "\n",
    "2. **Supervised Anomaly Detection:**\n",
    "   - **Training Data:** Supervised anomaly detection algorithms require a labeled dataset where each instance is explicitly marked as normal or anomalous.\n",
    "   - **Learning from Labels:** The algorithm is trained to distinguish between normal and anomalous instances based on the labeled examples provided during the training phase.\n",
    "   - **Detecting Anomalies:** Once trained, the model can be applied to new, unseen data to classify instances as normal or anomalous based on the learned patterns.\n",
    "\n",
    "   **Examples of Supervised Methods:**\n",
    "   - Support Vector Machines (SVM) with labeled data.\n",
    "   - Decision Trees or Random Forests trained on labeled data.\n",
    "   - Neural networks, such as deep learning models, with labeled data.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "- **Availability of Labeled Data:** Unsupervised methods do not rely on labeled data during training, making them suitable for scenarios where labeled anomalies are scarce or expensive to obtain. Supervised methods, on the other hand, require labeled data for training.\n",
    "\n",
    "- **Flexibility:** Unsupervised methods are more flexible and can adapt to changing patterns in the data without the need for re-labeling. In contrast, supervised methods may require retraining if the nature of anomalies evolves.\n",
    "\n",
    "- **Applicability:** Unsupervised methods are commonly used when anomalies are not well-defined or are difficult to label. Supervised methods are suitable when labeled examples of anomalies are available and the goal is to explicitly train a model to recognize them.\n",
    "\n",
    "- **Scalability:** Unsupervised methods can be more scalable to large datasets as they do not involve the manual labeling of data. Supervised methods may require substantial effort in labeling a representative set of anomalies.\n",
    "\n",
    "The choice between unsupervised and supervised anomaly detection depends on the availability of labeled data, the nature of anomalies, and the specific requirements of the application. In some cases, a hybrid approach, combining elements of both, may be employed to take advantage of available labeled data while adapting to evolving patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1253729",
   "metadata": {},
   "source": [
    "# Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ed4ff",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be categorized into several main types based on the underlying techniques and approaches they employ. Here are some of the main categories:\n",
    "\n",
    "1. **Statistical Methods:**\n",
    "   - **Description:** Statistical anomaly detection methods model the normal behavior of the system using statistical parameters such as mean, median, standard deviation, or other measures.\n",
    "   - **Examples:** Z-score, Gaussian Mixture Models (GMM), Moving Average, Exponential Smoothing.\n",
    "\n",
    "2. **Machine Learning-Based Methods:**\n",
    "   - **Description:** Machine learning algorithms are used to learn the patterns of normal behavior from the data, and anomalies are identified as instances deviating significantly from these learned patterns.\n",
    "   - **Examples:** Support Vector Machines (SVM), Decision Trees, Random Forests, k-Nearest Neighbors (k-NN), Neural Networks, Isolation Forests.\n",
    "\n",
    "3. **Clustering-Based Methods:**\n",
    "   - **Description:** These methods group similar data points into clusters, assuming that anomalies are data points that do not conform to any cluster.\n",
    "   - **Examples:** k-Means clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), hierarchical clustering.\n",
    "\n",
    "4. **Density-Based Methods:**\n",
    "   - **Description:** Density-based anomaly detection methods identify anomalies as instances that have significantly lower or higher density compared to their neighbors.\n",
    "   - **Examples:** LOF (Local Outlier Factor), One-Class SVM (Support Vector Machine), Autoencoders.\n",
    "\n",
    "5. **Distance-Based Methods:**\n",
    "   - **Description:** Anomalies are identified based on the distances between data points. Instances that are significantly distant from others are considered anomalies.\n",
    "   - **Examples:** Mahalanobis Distance, k-NN (k-Nearest Neighbors), Angle-Based Outlier Detection.\n",
    "\n",
    "6. **Time-Series Methods:**\n",
    "   - **Description:** These methods are specialized for detecting anomalies in time-series data and consider temporal dependencies and patterns.\n",
    "   - **Examples:** Seasonal decomposition of time series (STL), AutoRegressive Integrated Moving Average (ARIMA), Prophet.\n",
    "\n",
    "7. **Ensemble Methods:**\n",
    "   - **Description:** Ensemble methods combine multiple anomaly detection algorithms or models to enhance overall performance and robustness.\n",
    "   - **Examples:** Isolation Forests, Majority Voting Ensembles.\n",
    "\n",
    "8. **Deep Learning-Based Methods:**\n",
    "   - **Description:** Deep learning approaches, particularly autoencoders, are used to learn complex representations of normal behavior and identify anomalies based on deviations from these representations.\n",
    "   - **Examples:** Autoencoders, Variational Autoencoders (VAE), Long Short-Term Memory (LSTM) networks for time-series data.\n",
    "\n",
    "9. **Isolation Forests:**\n",
    "   - **Description:** Isolation Forests are a specific type of tree-based method that isolates anomalies by constructing trees and measuring the ease with which instances can be separated.\n",
    "\n",
    "10. **Rule-Based Methods:**\n",
    "    - **Description:** Rule-based anomaly detection involves defining explicit rules or thresholds to identify anomalies based on predefined conditions.\n",
    "    - **Examples:** Business rules, expert knowledge-based rules.\n",
    "\n",
    "The choice of which algorithm to use depends on the nature of the data, the characteristics of anomalies, the availability of labeled data, and the specific requirements of the application. Often, a combination of multiple methods or an ensemble approach is used to achieve better overall performance and adaptability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e04af2",
   "metadata": {},
   "source": [
    "# Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8801f2",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on the assumption that normal instances in a dataset tend to be close to each other in the feature space, while anomalies are significantly distant from the majority of normal instances. These methods make several key assumptions to identify anomalies based on the distances between data points. Here are the main assumptions:\n",
    "\n",
    "1. **Density-Based Assumption:**\n",
    "   - **Assumption:** Normal instances are densely clustered in the feature space, and anomalies have lower local density.\n",
    "   - **Rationale:** Anomalies are assumed to be isolated points or instances that do not conform to the dense neighborhoods typical of normal instances.\n",
    "\n",
    "2. **Neighbor-Based Assumption:**\n",
    "   - **Assumption:** Normal instances have similar neighbors in terms of feature values, and anomalies have dissimilar or sparse neighborhoods.\n",
    "   - **Rationale:** Anomalies are expected to have fewer similar instances in their proximity, leading to higher distances when measuring the similarity or dissimilarity of neighbors.\n",
    "\n",
    "3. **Distance Threshold Assumption:**\n",
    "   - **Assumption:** Anomalies can be identified by setting a distance threshold beyond which instances are considered abnormal.\n",
    "   - **Rationale:** Instances that are significantly distant from the majority of data points are likely to be anomalies. A threshold is set to classify instances as normal or anomalous based on their distances.\n",
    "\n",
    "4. **Homogeneous Density Assumption:**\n",
    "   - **Assumption:** Normal instances exhibit relatively homogeneous density across the feature space, while anomalies have uneven or sparse density.\n",
    "   - **Rationale:** Anomalies are expected to deviate from the homogeneous density pattern observed in normal instances, making them stand out.\n",
    "\n",
    "5. **Global Consistency Assumption:**\n",
    "   - **Assumption:** The overall structure of the dataset, when considered globally, is indicative of normal behavior, and anomalies disrupt this global consistency.\n",
    "   - **Rationale:** Anomalies are assumed to introduce global inconsistencies in the dataset, leading to higher overall distances compared to normal instances.\n",
    "\n",
    "6. **Distance Metric Assumption:**\n",
    "   - **Assumption:** The choice of distance metric is crucial, and anomalies can be effectively identified based on the distance measure used.\n",
    "   - **Rationale:** Different distance metrics (e.g., Euclidean distance, Mahalanobis distance) capture different aspects of similarity or dissimilarity between data points, and the effectiveness of anomaly detection relies on choosing an appropriate metric for the given data.\n",
    "\n",
    "It's important to note that the success of distance-based anomaly detection methods depends on the validity of these assumptions in the specific context of the data. These methods may struggle when data exhibits complex structures, non-uniform density, or when anomalies are not well-separated in the feature space. Careful consideration of the characteristics of the dataset and the nature of anomalies is crucial when applying distance-based anomaly detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d683a16",
   "metadata": {},
   "source": [
    "# Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4aeab",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm is a density-based anomaly detection method that assesses the local density of data points to identify anomalies. The LOF algorithm computes anomaly scores for each data point, indicating how much it deviates from the local density of its neighbors. Here's an overview of how LOF computes anomaly scores:\n",
    "\n",
    "1. **Local Reachability Density (LRD):**\n",
    "   - For each data point \\(p\\), LOF calculates its local reachability density (LRD). LRD is an estimation of the density of data point \\(p\\) relative to its neighbors. It is computed as the inverse of the average reachability distance of \\(p\\) with respect to its k-nearest neighbors.\n",
    "   - The reachability distance between two points \\(p\\) and \\(q\\) is the maximum of the distance between \\(p\\) and \\(q\\) and the k-distance of \\(q\\), where the k-distance is the distance to the k-th nearest neighbor of \\(q\\).\n",
    "   - Mathematically, the LRD for point \\(p\\) is calculated as follows:\n",
    "     \\[ LRD(p) = \\left(\\frac{1}{\\text{average reachability distance of } p \\text{ with respect to its k-nearest neighbors}}\\right)^{-1} \\]\n",
    "\n",
    "2. **Local Outlier Factor (LOF):**\n",
    "   - The LOF for each data point \\(p\\) is then calculated by comparing its LRD with the LRDs of its neighbors. The LOF for \\(p\\) is the average ratio of its LRD to the LRDs of its k-nearest neighbors.\n",
    "   - Mathematically, the LOF for point \\(p\\) is calculated as follows:\n",
    "     \\[ LOF(p) = \\frac{\\text{Average LRD of neighbors of } p}{LRD(p)} \\]\n",
    "\n",
    "3. **Anomaly Score:**\n",
    "   - The anomaly score for each data point is determined by the LOF. Higher LOF values indicate that the point has a lower local density compared to its neighbors, suggesting it may be an anomaly.\n",
    "   - Anomaly scores are often normalized, and a threshold can be set to classify points as normal or anomalous based on their LOF values.\n",
    "\n",
    "In summary, LOF computes the anomaly score for each data point by assessing its local density relative to its neighbors. Points with higher LOF values are considered more likely to be anomalies. LOF is particularly effective in identifying anomalies in datasets with varying densities and complex structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d5145",
   "metadata": {},
   "source": [
    "# Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad24a8",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is an unsupervised machine learning algorithm for anomaly detection. It is based on the idea that anomalies are typically isolated instances and can be detected more easily than normal instances. The key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "1. **Number of Trees (n_estimators):**\n",
    "   - **Description:** This parameter specifies the number of isolation trees to build. Increasing the number of trees can improve the accuracy of the algorithm but may also increase computation time.\n",
    "   - **Default:** Typically set to 100.\n",
    "\n",
    "2. **Maximum Samples (max_samples):**\n",
    "   - **Description:** It determines the maximum number of samples to be used when constructing each isolation tree. A smaller value can speed up the training process but may result in less accurate models.\n",
    "   - **Default:** Often set to \"auto,\" which means it uses the minimum of 256 and the total number of samples.\n",
    "\n",
    "3. **Contamination:**\n",
    "   - **Description:** The estimated proportion of outliers in the dataset. It is used to set the threshold for classifying instances as anomalies. A higher contamination value leads to a lower threshold, classifying more instances as anomalies.\n",
    "   - **Default:** Typically set to 0.1, but it depends on the characteristics of the dataset.\n",
    "\n",
    "4. **Bootstrap:**\n",
    "   - **Description:** If set to True, each isolation tree is built using a bootstrap sample of the data (sampling with replacement). If set to False, the entire dataset is used for each tree.\n",
    "   - **Default:** True.\n",
    "\n",
    "5. **Random Seed (random_state):**\n",
    "   - **Description:** It is used to seed the random number generator for reproducibility. Setting a specific random seed ensures that the algorithm produces the same results when run multiple times with the same parameters.\n",
    "   - **Default:** None (no fixed seed).\n",
    "\n",
    "These parameters allow users to control the behavior of the Isolation Forest algorithm and tailor it to the characteristics of the dataset at hand. Proper tuning of these parameters is essential for achieving good performance in anomaly detection tasks. The default values often work well for many scenarios, but adjustments may be necessary depending on the specific characteristics of the data being analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb512b8e",
   "metadata": {},
   "source": [
    "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235aeaa3",
   "metadata": {},
   "source": [
    "To calculate the anomaly score of a data point using the k-Nearest Neighbors (k-NN) algorithm, we need to consider the local density around the point. The anomaly score is often determined by the distance or density of the data point relative to its k-nearest neighbors. In this case, you've mentioned that the data point has only 2 neighbors of the same class within a radius of 0.5, and we are using k-NN with \\(K = 10\\).\n",
    "\n",
    "The anomaly score in k-NN is often associated with the distance to the \\(K\\)-th nearest neighbor. If a data point has only 2 neighbors within a radius of 0.5, and \\(K = 10\\), it means there are not enough neighbors within the specified range to consider the \\(K\\)-th nearest neighbor.\n",
    "\n",
    "In a typical scenario with k-NN, you would consider the distances to the \\(K\\)-th nearest neighbor (where \\(K\\) is a specified parameter) to evaluate the anomaly score. However, if there are only 2 neighbors available and \\(K = 10\\), you won't be able to calculate the distance to the 10th neighbor in this case.\n",
    "\n",
    "To assess the anomaly score in a more accurate way, you would typically choose \\(K\\) to be less than or equal to the number of available neighbors, or adjust \\(K\\) based on the characteristics of your dataset. If there are only 2 neighbors within a radius of 0.5, using \\(K = 2\\) would be appropriate. Then, you could calculate the anomaly score based on the distances to the 2nd nearest neighbor. The score would be higher if the distance is large, indicating that the point is less similar to its neighbors and potentially more anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a97c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 14.422205101855956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def calculate_anomaly_score(data, point, k=5):\n",
    "    # Ensure that k is less than or equal to the number of samples\n",
    "    k = min(k, len(data))\n",
    "\n",
    "    # Fit a k-NN model on the data\n",
    "    knn_model = NearestNeighbors(n_neighbors=k)\n",
    "    knn_model.fit(data)\n",
    "\n",
    "    # Find the distances and indices of the k-nearest neighbors for the given point\n",
    "    distances, indices = knn_model.kneighbors([point])\n",
    "\n",
    "    # The anomaly score can be calculated as the distance to the k-th nearest neighbor\n",
    "    anomaly_score = distances[0][-1]\n",
    "\n",
    "    return anomaly_score\n",
    "\n",
    "# Example usage\n",
    "data = [[1.0, 2.0], [1.2, 2.2], [2.0, 3.0], [10.0, 15.0], [10.5, 15.5]]\n",
    "\n",
    "# Assuming the point [2.5, 3.5] is the one for which we want to calculate the anomaly score\n",
    "point_to_check = [2.5, 3.5]\n",
    "\n",
    "anomaly_score = calculate_anomaly_score(data, point_to_check, k=5)\n",
    "print(f\"Anomaly Score: {anomaly_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26649c8d",
   "metadata": {},
   "source": [
    "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960252e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "def calculate_anomaly_score(avg_path_length, c):\n",
    "    anomaly_score = 2 ** (-avg_path_length / c)\n",
    "    return anomaly_score\n",
    "\n",
    "# Example usage\n",
    "avg_path_length = 5.0\n",
    "\n",
    "# Assuming c is known (replace it with the actual value)\n",
    "c = 10.0\n",
    "\n",
    "anomaly_score = calculate_anomaly_score(avg_path_length, c)\n",
    "print(f\"Anomaly Score: {anomaly_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89e19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
